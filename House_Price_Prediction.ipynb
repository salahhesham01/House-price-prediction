{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salahhesham01/House-price-prediction/blob/main/House_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part1"
      ],
      "metadata": {
        "id": "uPlaointwq77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Data Exploration"
      ],
      "metadata": {
        "id": "-Qq_85yLnHbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pandas profilling\n",
        "!pip install ydata-profiling\n"
      ],
      "metadata": {
        "id": "eabuBj-nnPVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport"
      ],
      "metadata": {
        "id": "0AJSG4toZl65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import boxcox\n",
        "import math, copy"
      ],
      "metadata": {
        "id": "XDgG13nqbDnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "original_data = pd.read_csv('/content/drive/MyDrive/Assignment1/house-prices-advanced-regression-techniques/train.csv')"
      ],
      "metadata": {
        "id": "Q8jss-W9bD0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_data = original_data.copy()"
      ],
      "metadata": {
        "id": "XCtpSjgvbEFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = ProfileReport(copy_data, minimal=True)\n",
        "report"
      ],
      "metadata": {
        "id": "FMQ1fLdWcsFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Data Cleaning"
      ],
      "metadata": {
        "id": "Vt78b_e7nLuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check and deal with all the missing values in the data\n",
        "# check and deal with duplicated rows in the data\n",
        "# check and deal with outliers\n",
        "# Remove any variable that is uniformally distributed"
      ],
      "metadata": {
        "id": "JK9NQFAQnPpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_data"
      ],
      "metadata": {
        "id": "gNZH3TZIeXYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check and deal with all the missing values in the data\n",
        "copy_data = copy_data.drop(['Alley','PoolQC','Fence' , 'MiscFeature'],axis = 1)\n",
        "for col in copy_data.columns:\n",
        "    if copy_data[col].dtype in ['int64', 'float64']:\n",
        "        copy_data[col].fillna(copy_data[col].mean(), inplace=True)\n",
        "    elif copy_data[col].dtype == 'category':\n",
        "        copy_data[col].fillna(copy_data[col].mode(), inplace=True)"
      ],
      "metadata": {
        "id": "1h0QpnDAcw_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_data.drop(columns=['Id'], inplace=True)"
      ],
      "metadata": {
        "id": "kBczzsoSmroc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check and deal with duplicated rows in the data\n",
        "copy_data = copy_data.drop_duplicates(keep=False)"
      ],
      "metadata": {
        "id": "WMHLBjErcx-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any variable that is uniformally distributed\n",
        "for col in copy_data.columns:\n",
        "    if copy_data[col].nunique() <= 1:\n",
        "       copy_data.drop(col, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "VcVKmbtoc0RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Feature Engineering"
      ],
      "metadata": {
        "id": "h_yEl1HunO3S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rAy3b5UnEhN"
      },
      "outputs": [],
      "source": [
        "# create at least one new feature and check it's relationship with the target variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_season(row):\n",
        "  if row.MoSold in range (1,3):\n",
        "    return 'Winter'\n",
        "  elif row.MoSold in range(3,6):\n",
        "    return 'Spring'\n",
        "  elif row.MoSold in range (6,9):\n",
        "    return 'Summer'\n",
        "  elif row.MoSold in range (9,12):\n",
        "    return 'Fall'\n",
        "  else:\n",
        "    return 'Winter'\n",
        "copy_data['Season'] = copy_data.apply(get_season, axis=1)\n",
        "copy_data['Season']"
      ],
      "metadata": {
        "id": "XAxlS2jec2uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create at least one new feature and check it's relationship with the target variable\n",
        "copy_data['HouseAge']=copy_data['YrSold']-copy_data['YearBuilt']\n",
        "copy_data['HouseAge']"
      ],
      "metadata": {
        "id": "KP7zeG_Pc3BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Data Preprocessing"
      ],
      "metadata": {
        "id": "kl0ECwoQnULU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode catigorical variables with one hot encoding or label encoding based on the type of the variable\n",
        "\n",
        "ord_col=['FireplaceQu','ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond']\n",
        "# one hot encoding to nominal\n",
        "for col in copy_data.columns:\n",
        "    if copy_data[col].dtype == 'object' and col not in ord_col:\n",
        "        one_hot = pd.get_dummies(copy_data[col], prefix=col)\n",
        "        copy_data = copy_data.drop(col, axis=1)\n",
        "        copy_data = copy_data.join(one_hot)\n",
        "#label encoding to ordinal\n",
        "le = LabelEncoder()\n",
        "for col in ord_col:\n",
        "    copy_data[col] = le.fit_transform(copy_data[col])"
      ],
      "metadata": {
        "id": "UvtUZvnVc8-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use minmax scaler or standarad scaler to make all numerical variables within the same range\n",
        "normalizer = MinMaxScaler()\n",
        "numerical_cols = copy_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "copy_data[numerical_cols] = normalizer.fit_transform(copy_data[numerical_cols])"
      ],
      "metadata": {
        "id": "PrVKiDfQf4b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try to make at least one variable that is not following normal distribution to be normally distributed\n",
        "copy_data['YearBuilt'] = copy_data['YearBuilt'] + 1\n",
        "copy_data['YearBuilt'], _ = boxcox(copy_data['YearBuilt'])"
      ],
      "metadata": {
        "id": "rdx3ZWGNf5BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part2"
      ],
      "metadata": {
        "id": "IT21bwfdxhze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Training And Evaluation"
      ],
      "metadata": {
        "id": "z7Dbq9UGnXjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split your train data to be 80% training and 20% validation\n",
        "# use l2 regularization to reduce overfitting\n",
        "# use early stopping to reduce overfitting\n",
        "# train with at least 2 different learning rates and decide which experiment was better\n",
        "# print the validation root meen square error after finishing the training with the best model"
      ],
      "metadata": {
        "id": "RwoBfwCLnbZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split your train data to be 80% training and 20% validation\n",
        "X_train = copy_data.drop('SalePrice',axis=1)"
      ],
      "metadata": {
        "id": "P9rpc4w4kVH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = copy_data['SalePrice']"
      ],
      "metadata": {
        "id": "AjN3TMWbkWRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and validation sets with shuffling\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=1)"
      ],
      "metadata": {
        "id": "jQzWrU_Vk3vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "for col_name in list(X_train.columns.values):\n",
        "  X_train[col_name] = scaler.fit_transform(X_train[[col_name]])\n",
        "  X_val[col_name] = scaler.transform(X_val[[col_name]])"
      ],
      "metadata": {
        "id": "9-55A_qxpH3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "\n",
        "X_val = X_val.to_numpy()\n",
        "y_val = y_val.to_numpy()"
      ],
      "metadata": {
        "id": "-OD0H7pkphoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_linear_reg(X, y, w, b, lambda_):\n",
        "\n",
        "    m  = X.shape[0]\n",
        "    n  = len(w)\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b\n",
        "        cost = cost + (f_wb_i - y[i])**2\n",
        "    cost = cost / (2 * m)\n",
        "\n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost\n",
        "    total_cost = cost + reg_cost\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "GngNv322pqnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regularized_gradient_function(X, y, w, b, lambda_):\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "uQApQjML4eBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regularized_gradient_descent_with_early_stopping(X_train, y_train, X_val,y_val, w_in, b_in, cost_function, gradient_function, alpha, num_iters,lambda_, early_stopping_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
        "    num_iters gradient steps with learning rate alpha and regularization lambda\n",
        "    \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history_train = []\n",
        "    J_history_validation = []\n",
        "    w_histroy = []\n",
        "    b_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    counter = 0              #Counting the number of consective iterations that the validation cost is not improving in them\n",
        "    min_validation_cost = 10000000 #adding initial very big number\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = regularized_gradient_function(X_train, y_train, w, b, lambda_)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        train_cost = compute_cost_linear_reg(X_train, y_train, w, b, lambda_)\n",
        "        J_history_train.append(train_cost)\n",
        "\n",
        "        validation_cost = compute_cost_linear_reg(X_val, y_val, w, b, lambda_)\n",
        "        J_history_validation.append(validation_cost)\n",
        "        if validation_cost >= min_validation_cost:\n",
        "          counter += 1\n",
        "          if counter == early_stopping_iters:\n",
        "            print(\"Early Stopping Reached In Iteration:\",i)\n",
        "            break\n",
        "        else:\n",
        "          min_validation_cost = validation_cost\n",
        "          counter = 0\n",
        "\n",
        "\n",
        "\n",
        "        w_histroy.append(w)\n",
        "        b_history.append(b)\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% 100 == 0:\n",
        "            print(f\"Iteration {i:4d} -- Train-Cost {J_history_train[-1]:8.2f}   -- Validation-Cost {J_history_validation[-1]:8.2f}  \")\n",
        "\n",
        "    return w_histroy[np.argmin(J_history_validation)], b_history[np.argmin(J_history_validation)], J_history_validation #return best w,b and J history for graphing"
      ],
      "metadata": {
        "id": "srk61_yj4mWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_b =200\n",
        "num_features = X_train.shape[1]\n",
        "initial_w = np.zeros(num_features)\n",
        "# some gradient descent settings\n",
        "num_iters = 10000\n",
        "alpha = 0.01\n",
        "lambda_ = 0.001  #It's common to use lambda value from 0 to 0.1\n",
        "early_stopping_iters = 10\n",
        "\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist = regularized_gradient_descent_with_early_stopping(X_train, y_train, X_val,y_val, initial_w, initial_b, compute_cost_linear_reg, regularized_gradient_function, alpha, num_iters,lambda_, early_stopping_iters)"
      ],
      "metadata": {
        "id": "bYT8qeOhWM9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "\n",
        "    m  = X.shape[0]\n",
        "    y_pred = np.zeros((m,))\n",
        "    for i in range(m):\n",
        "        y_pred[i] = np.dot(X[i], w) + b\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "3ORf5ahCxLBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X: {X_val[0]}\")\n",
        "print(f\"W {w_final}\")\n",
        "print(f\"b {b_final}\")\n",
        "\n",
        "# make a prediction\n",
        "f_wb = predict(X_val,w_final,b_final)\n",
        "print(f\"Prediction: {f_wb[0]}\")"
      ],
      "metadata": {
        "id": "xe7aTaimxNX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = predict(X_val, w_final, b_final)\n",
        "\n",
        "# Calculate RMSE\n",
        "mse = mean_squared_error(y_val, y_val_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f'Validation RMSE: {rmse}')"
      ],
      "metadata": {
        "id": "QlL3Zb93pnPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Assignment1/house-prices-advanced-regression-techniques/test.csv')\n",
        "test_data=data.copy()"
      ],
      "metadata": {
        "id": "U8r0oygdxza8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and deal with missing values\n",
        "test_data = test_data.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
        "for col in test_data.columns:\n",
        "    if test_data[col].dtype in ['int64', 'float64']:\n",
        "        test_data[col].fillna(test_data[col].mean(), inplace=True)\n",
        "    elif test_data[col].dtype == 'category':\n",
        "      test_data[col].fillna(test_data[col].mode(), inplace=True)\n",
        "\n",
        "test_data.drop(columns=['Id'], inplace=True)\n",
        "# Check and deal with duplicated rows\n",
        "test_data = test_data.drop_duplicates(keep=False)\n",
        "\n",
        "# Remove any variable that is uniformly distributed\n",
        "for col in test_data.columns:\n",
        "   if test_data[col].nunique() <= 1:\n",
        "       test_data.drop(col, axis=1, inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "test_data['HouseAge'] = test_data['YrSold'] - test_data['YearBuilt']\n",
        "\n",
        "# Data Preprocessing\n",
        "for col in test_data.columns:\n",
        "    if test_data[col].dtype == 'object' and col not in ord_col:\n",
        "        one_hot = pd.get_dummies(test_data[col], prefix=col)\n",
        "        test_data = test_data.drop(col, axis=1)\n",
        "        test_data = test_data.join(one_hot)\n",
        "\n",
        "for col in ord_col:\n",
        "    test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "# Try to make at least one variable that is not following a normal distribution to be normally distributed\n",
        "test_data['YearBuilt'] = test_data['YearBuilt'] + 1\n",
        "test_data['YearBuilt'], _ = boxcox(test_data['YearBuilt'])\n",
        "\n",
        "X_test = test_data\n",
        "\n",
        "# Standardize numerical features\n",
        "for col_name in list(X_test.columns.values):\n",
        "    X_test[col_name] = scaler.fit_transform(X_test[[col_name]])"
      ],
      "metadata": {
        "id": "Z0-ihrcoQTxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Validation RMSE: {rmse}')"
      ],
      "metadata": {
        "id": "AliTNn92bkcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.Submission"
      ],
      "metadata": {
        "id": "WA78TO44neq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on the test set, create a csv file and submit on kaggle"
      ],
      "metadata": {
        "id": "13XLl-iXoqTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cols = set(copy_data.columns) - set(X_test.columns)\n",
        "\n",
        "# Add a missing column in test set with default value equal to 0\n",
        "for c in missing_cols:\n",
        "    X_test[c] = 0\n",
        "\n",
        "# Ensure the order of column in the test set is in the same order than in train set\n",
        "X_test = X_test[copy_data.columns]"
      ],
      "metadata": {
        "id": "qNI7JNyffzCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to NumPy arrays\n",
        "X_test = X_test.to_numpy()"
      ],
      "metadata": {
        "id": "feomDVQFhIPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.delete(X_test, -1, axis=1)  # This will remove the last column\n"
      ],
      "metadata": {
        "id": "p-cEpZyfmNLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = predict(X_test, w_final, b_final)"
      ],
      "metadata": {
        "id": "xY2eGR_tbMFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9S7IskXVchLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    \"Id\": test_data.index+1461,  # Assuming the test data has an 'Id' column\n",
        "    \"SalePrice\": y_test_pred\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "1iO1cfFkjL_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R9vqM60lcbrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h8ap_v0v-QMf"
      }
    }
  ]
}